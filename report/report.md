# üìä Relat√≥rio ‚Äì Classifica√ß√£o de Texto com Random Forest

## 1. Introdu√ß√£o
O objetivo deste trabalho √© avaliar o desempenho de um **classificador Random Forest** aplicado a tr√™s bases de dados de **classifica√ß√£o de texto supervisionada**, contendo avalia√ß√µes e mensagens anotadas com classes **positivas (1)** ou **negativas (0)**.  

As bases utilizadas foram:  
- **Amazon Cells Labelled**  
- **Yelp Labelled**  
- **IMDB Labelled**  

Todas possuem formato `text<TAB>target` e foram processadas no **Google Colab**, utilizando **TF-IDF** para vetoriza√ß√£o dos textos.

---

## 2. Metodologia
As etapas seguidas foram:

1. **Carregamento das bases** no Colab via `files.upload()`.  
2. **Padroniza√ß√£o das colunas** para `text` e `target`.  
3. **An√°lise de balanceamento** das classes.  
4. **Divis√£o** em treino (75%) e teste (25%) utilizando `train_test_split`.  
5. **Vetoriza√ß√£o TF-IDF** para transformar o texto em espa√ßo vetorial.  
6. **Treinamento** com `RandomForestClassifier` (par√¢metros padr√£o).  
7. **Avalia√ß√£o** com m√©tricas:
   - Accuracy (acur√°cia)  
   - Precision (precis√£o)  
   - Recall (revoca√ß√£o)  
   - AUC (√°rea sob a curva ROC)  
8. **Plotagem da curva ROC** para an√°lise visual do desempenho.  

---

## 3. Resultados

### 3.1. Balanceamento das Classes
Todas as bases apresentam classes relativamente balanceadas entre positivo (1) e negativo (0), com pequenas varia√ß√µes.

| Base   | Classe 0 (%) | Classe 1 (%) |
|--------|-------------|--------------|
| Amazon | 50%         | 50%          |
| Yelp   | 50%         | 50%          |
| IMDB   | 48.3%       | 51.6%        |

---

### 3.2. M√©tricas de Avalia√ß√£o

| Base   | Accuracy | Precision | Recall | AUC   |
|--------|----------|-----------|--------|-------|
| Amazon | 0.748    | 0.756     | 0.782  | 0.876 |
| Yelp   | 0.780    | 0.800     | 0.720  | 0.820 |
| IMDB   | 0.695    | 0.677     | 0.714  | 0.760 |

- **Amazon**:  
  A acur√°cia de **0.748** mostra que o modelo acerta em m√©dia 74,8% das previs√µes.  
  A precis√£o de **0.756** indica que, quando o modelo prev√™ positivo, ele acerta 75,6% das vezes.  
  O recall de **0.782** mostra que consegue recuperar a maioria dos exemplos realmente positivos.  

- **Yelp**:  
  Apresentou a **maior acur√°cia (0.78)**, com precis√£o de **0.80**, indicando bom desempenho ao prever positivos com baixa taxa de falsos positivos.  
  O recall menor (**0.72**) sugere que alguns positivos foram perdidos.  

- **IMDB**:  
  Foi a base mais desafiadora, com acur√°cia de **0.69**.  
  A precis√£o (**0.67**) indica maior erro ao classificar positivos, e o recall (**0.71**) mostra que o modelo n√£o recupera t√£o bem todos os positivos.  
  Isso pode estar ligado ao vocabul√°rio mais variado dos textos de filmes.  

- **AUC**:  
  Em todas as bases os valores foram satisfat√≥rios (acima de **0.75**), mostrando que o modelo tem boa capacidade de separar classes positivas e negativas.  
  Melhor resultado: **Amazon (0.876)**.  
  Pior resultado: **IMDB (0.760)**.  

---

### 3.3. Curvas ROC

üìå **Figura 1 ‚Äì Curva ROC para a base Amazon**  
<img width="578" height="455" alt="ROC Amazon" src="https://github.com/user-attachments/assets/b72f7736-3351-49f8-bec6-f04e1cd64b11" />

üìå **Figura 2 ‚Äì Curva ROC para a base Yelp**  
<img width="578" height="455" alt="ROC Yelp" src="https://github.com/user-attachments/assets/f12f0b59-a911-42c7-9611-3c4eaa680cd0" />

üìå **Figura 3 ‚Äì Curva ROC para a base IMDB**  
<img width="578" height="455" alt="ROC IMDB" src="https://github.com/user-attachments/assets/1ccd666a-e7d8-4bfe-a847-3468303331d0" />

---

## 4. Conclus√£o
Com base nos resultados apresentados:  

- O modelo **Random Forest** apresentou desempenho consistente, com acur√°cia entre **70% e 78%**.  
- Como as classes est√£o **balanceadas**, a acur√°cia √© confi√°vel, sem vi√©s por distribui√ß√£o.  
- A precis√£o mais alta (Amazon e Yelp) mostra que o modelo foi eficaz em **evitar falsos positivos**.  
- O **recall da Amazon** foi o melhor, indicando que essa base era mais previs√≠vel em termos de vocabul√°rio.  
- A **IMDB foi a mais dif√≠cil**, devido √† maior variedade de termos.  
- A **AUC confirma** que o modelo separa bem as classes, mesmo com varia√ß√µes entre bases.  

üìå **Resumo**: O pipeline **TF-IDF + Random Forest** √© adequado para classifica√ß√£o bin√°ria de textos curtos, mas o desempenho depende fortemente do vocabul√°rio e do contexto de cada dataset.

---
